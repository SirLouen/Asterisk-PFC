% -*-desarrollo.tex-*-
% Este fichero es parte de la plantilla LaTeX para
% la realización de Proyectos Final de Carrera, protejido
% bajo los términos de la licencia GFDL.
% Para más información, la licencia completa viene incluida en el
% fichero fdl-1.3.tex

\section{Elección del Servidor}

A la hora de plantearse que Servidor va a ser más adecuado para nuestro sistema aunque tenemos múltiples opciones, hay que tener presentes varios aspectos. Hay que considerar que aunque Asterisk, sea ámpliamente reconocido por funcionar en cualquier sistema, dado que realmente se trata de un software que funciona bajo un sistema de tipo *NIX, el rendimiento no depende tanto en exclusiva de las prestaciones de la máquina sino de las posible incidencias colaterales, y a su vez de ciertos aspectos que deben preservar un sistema de funcionamiento crítico como es el sistema de telefonía en términos generales (Considerando que hasta hoy en día, ni siquiera una persona física puede prescindir un solo día de su teléfono en activo).

Entre los equipos más populares destacan las marcas Dell y HP por su extensiva y conocida fiabilidad entre la comunidad Asterisk. Es bien conocido uno de los problemas populares surgidos en general con los servidores, dificultades con el sistema de interrupciones IRQ, dado que algunos servidores de alta producción pueden llegar a contener múltiples tarjetas PCI de comunicaciones  y hacer un uso intensivo de las mismas. Pero también hay que considerar que hoy en día, gracias a las nuevas tarjetas PCI-x y también la posibilidad de realizar las comunicaciones a través de dispositivos externos al servidor (Gateways) ya no es esto tan crucial. Por eso a nivel de marcas, siguen prevaleciendo estas dos, pero solo por el soporte de calidad que ofrecen. 

Considerando la opción Dell por la relación calidad-precio principalmente, lo  correcto sería plantearse aun el más bajo de gama, algo con prestaciones exclusivas de servidor. Por un lado la posibilidad de tener fuentes de alimentación redundantes, además de tener dos o cuatro sockets para múltiples procesadores, y una placa base con opción a poner memorias registradas. En otro caso hasta un equipo MIY, podría hacerle la competencia a cualquier servidor de Marca como veremos mas adelante.

Otro aspecto interesante, pero no imprescindible, es la capacidad de acceder a una tarjeta especifica de las marcas para realizar un sistema RAID por hardware, y opcionalmente, la disponibilidad especifica, de cajas tipo enrackables para adaptarse óptimamente a nuestros respectivos CPD.

Me gustaría ofrecer una opinión personal en términos generales. No merece la pena gastarse el dinero que cuesta un servidor si no vamos a tener las prestaciones especiales que ofrece una placa de servidor y todo lo que le rodea (posibilidad de fuentes redundantes, memorias registradas, dos o cuatro sockets, hotswaps, controladoras RAID de alta gama, etc.). Hoy en día por un bajo coste (entre 1000 y 2000 euros), podemos tener un equipo del nivel de un servidor. Más curioso aun, los servidores en formato de caja de torre de las marcas, no aportan un precio mejorado a mayores prestaciones, si bien, no requieren de rack, en caso de poseerlo, es hasta un mayor inconveniente de espacio y desorden. De hecho hoy en día están surgiendo empresas que se dedican a ensamblar ordenadores eligiendo cuidadosamente los componentes más robustos del mercado y con ello implementar sistemas con una tasa muy baja de retorno \footnote{algunas PYME ya reportan con este sistema una reducción significativa de un 0.5\% de RMA sobre el total de equipos ensamblados a medida}.

Para entornos de producción pero con necesidades reducidas, quizá la opción más interesante, podría ser  tener un servidor de bajo nivel tipo MIY y con opción de registrar todos los terminales en otro servidor paralelo redundante, a través de comunicaciones de datos, en caso de avería.

Para este PFC, me planteo un servidor cualquiera de pruebas, y hacer experimentos.

Pero me gustaría ir un poco más allá. La primera “practica” va a ser montar un pequeño servidor de Producción que cumpla las funciones generales que aportan los servidores Asterisk: Servidor de FAX, Sistema IVR, un pequeño Call-Center para dar soporte a la empresa a nivel IT a través de la VoIP y las líneas analógicas que poseeremos, etc…

Para ello cuento con un equipo dedicado a efectos de servidor, con los siguientes componentes internos, procesador con 4 núcleos, 8Gb de RAM y 2 discos duros SATA2 (para funciones de RAID), y demás componentes clásicos.

Además, conectada internamente posee una tarjeta de bajo calibre, una Digium Wildcard B410P con 1 puerto FXO y 1 puerto FXS (para acceso telefonía analógica PSTN).

\section{Elección e instalación del Sistema Operativo en el Servidor}

Como he comentado anteriormente, una de las “bondades” de Asterisk es que aun no siendo realmente multiplaforma, debido a que esta diseñado exclusivamente para sistemas *NIX, dado el basto abanico que disponemos para este tipo de sistemas, existen unos mas interesantes que otros en cuestiones especificas de optimización de recursos. Hoy en día el 99\% de las instalaciones de Asterisk (y de otros tipos de sistemas servidor, como servidores ftp, web, etc.), se realizan sobre sistemas GNU/Linux. Pero para este apartado, el tema a tocar, seria concretamente la decisión de hacerlo, ¿sobre que distribución?

Siguiendo la línea, realmente hoy en día todas las distribuciones son validas, incluso distribuciones basadas en UNIX, desde Open Solaris, incluso distribuciones para dispositivos con un sistema operativo embebido como openWRT, y evidentemente, cualquier otra distribución GNU/Linux especifica, tipo RedHat, OpenSuse, Fedora y demás.

Pero en el mundo Asterisk las distribuciones mas destacadas por la comunidad son CentOS y Debian por igual. De todas formas esto no resulta nada nuevo ya que hoy en día, CentOS y Debian se reparten el pastel de los servidores, incluso por encima de Fedora y RedHat Enterprise.

Debian es para mí personalmente, la distribución que mas he tenido la oportunidad de manejar. Y además entre la comunidad de desarrollo, Debian esta cogiendo cada vez más popularidad, aunque existen ciertas Distribuciones, especificas de Asterisk y cada vez más reconocidas como Elastix, o Asterisk NOW! basadas en CentOS. 

Pero realmente, existe una distribución dentro del mundo Debian que simplifica masivamente la vida de configuración a nivel Hardware, Ubuntu en su versión especifica para servidores, Ubuntu Server. Es cierto que a día de hoy no es una personificación del máximo rendimiento y la optimización de los recursos del sistema ya que de por si añade un consumo “cabecera” (overhead) a la CPU que no es nada positivo para un sistema con una función especifica, como cumplirá en este caso lo que aquí estamos tratando, un sistema PBX.

Pero ahora volviendo atrás al momento de la decisión de configuración de Hardware me encontraba con una simple cuestión: nuestra maquina de pruebas, tampoco iba a servir como un sistema de propósito específico y de alto rendimiento. A efectos prácticos de prueba, no es la intención servir como Call Center para cubrir 500 llamadas simultaneas. Además Asterisk infraescalado en prestaciones, ni aun con un sistema como Debian en su versión mas ligera, seria capaz de soportar esto. Es por esto quizá, por lo que la facilidad de autoadaptación del Hardware en el sistema fue por ultimo mi decisión final de utilizar Ubuntu Server como sistema.

El instalador, en términos generales, es relativamente sencillo de seguir, y tras instalar Ubuntu Server pude comprobar como prácticamente todo el Hardware fue reconocido, y autoconfigurado, desde la Controladora RAID, hasta la VGA en alta resolución (no es gran cosa, pero personalmente me resulta muy útil trabajar en 1920x1080 incluso en consola si el sistema ofrece esta posibilidad como es el caso. Evidentemente, solo faltaba por reconocer la tarjeta Digium B410P descrita anteriormente.

La segunda parte quizá mas significativa de la configuración del sistema fue la del planteamiento de estabilidad del sistema a nivel de Disco Duro. En este ejemplo disponemos de dos discos duros de semejantes características, por lo que se veía motivada la necesidad de montar un RAID. Es determinante, que la mayoría de las placas base, a pesar de traer un controlador RAID, realmente no es un controlador puro y dedicado como podrían ser los controladores PERC de Dell. Considero un controlador dedicado, a ese controlador capaz de liberar a la CPU de la carga de realizar todas las tareas para que el RAID se viera satisfecho. En este caso mi intención era montar un RAID 1 (tipo Espejo).

En general la mayoría de las placas base suelen traer controladores Silicon Image, ATI Nvidia, VIA, etc., pero realmente no dejan de ser controladores de tipo software, gestionados desde la BIOS con solo las características básicas para poder proveer de información suficiente de como debería trabajar (pero no capaces de trabajar autónomamente). Esto podría resultar interesante, excepto por una cuestión muy importante: Si esta controladora se averiase (o mas común, si la misma placa se averiase), necesitaríamos para recomponer ese RAID una controladora exactamente igual, o de semejantes características, capaz de recomponer eso. Hoy en día, esto puede resultar excesivamente complejo, ya que la mayoría de las piezas quedan descatalogadas en poco menos de 2 años, y pese a que continua su producción, a partir de los 4 años es prácticamente imposible conseguir una a no ser que la busquemos a través de un Bróker, o que la pidamos a la marca bajo demanda (con un coste 10 veces mas caro de lo que costó originalmente repercutiendo en definitiva sobre el TCO del sistema severamente). Esta dependencia al Hardware, para mi me resulta inviable. Si tuviéramos la opción de tener controladoras especificas como las PERC de Dell que comentaba antes, que siguen ciertos estándares al menos dentro de la misma marca, y que su producción se extiende mas allá de los 5 años, además de ofrecer un hardware dedicado y liberando a la CPU de esta carga, entonces definitivamente si podría decidir, que merece totalmente la pena.

Entonces, en este caso, teniendo dos discos duros iguales, y la intención es montar un RAID 1, la solución bajo mi criterio más fiable nos la ofrece el propio sistema operativo. GNU/Linux provee la solución, Md Raid, con mdadm. El RAID software por excelencia.

Para configurar esto en un sistema de pequeña escala la idea es muy sencilla:

Ubuntu ofrece en el propio instalador la opción de configurar esto de manera grafica y eventualmente es muy práctico ya que se establece justo desde el momento de la inserción del primer dato del sistema, pero no necesario, ya que mdraid provee de un sistema de sincronización desde el momento de la puesta en marcha. En cualquier caso, por si a alguien le interesara configurarlo en modo Consola el concepto lo describo a continuación:

1. Para cada disco tenemos que crear primero una partición de tipo RAID arrancable (exactamente iguales, ocupando el disco completo, y con el flag B de bootable).

Podríamos utilizar una herramienta de particionado tipo cfdisk. Simplemente, toca, crear una nueva partición con \negrita{New}, en Tipo, ponerle \negrita{fd} (Particion Linux Raid), en flag, ponerle \negrita{bootable}, y finalmente “\negrita{Write}” para escribir las modificaciones sobre la partición. Repetir este proceso para el otro disco. 

2. En segundo lugar, toca “instalar” el Raid software con mdadm. En el instalador simplemente hay que especificar algunos parámetros, como el tipo de raid (1, mirror, espejado, misma información en los dos discos duros), el numero de discos involucrados, el numero de discos sobrantes (Spares, como en el sistema JBOD por si acaso) y seleccionar los discos involucrados. Y el sistema raid estará listo en unos segundos.

En el caso shell es quizá aun más sencillo:

Considerando que los discos son SDA y SDB (sata disk A y sata disk B siendo ambos discos tipo sata, si fuera IDE/PATA, seria hda, y hdb)

mdadm –create /dev/md0 –level=1 –raid-devices=2 /dev/sda1 /dev/sdb1 (el 1 es la partición 1 que creamos anteriormente, una sola partición).

En este caso habremos llamado a nuestro recién creado RAID md0 a nivel de sistema.

3. Acto seguido, toca particionar nuestro recién creado RAID. En este caso, seria como si tuviéramos un solo disco duro. Podríamos directamente particionarlo con ReiserFS, Ext3 o Ext4, o podríamos elegir un formato mas avanzado como LVM2. Yo personalmente soy más partidario de utilizar LVM por la opción de hacer una copia de seguridad del sistema completo utilizando Snapshots. Un RAID 1 aunque ayuda al uptime del servidor, ya que aunque haya un error de disco, el sistema seguirá en pie, pero no asegura que la información se destruya en un momento determinado (o peor aun, se corrompa). La única forma de preservar la fiabilidad de la información es utilizar las copias de seguridad. Y los snapshots de LVM son la forma idónea de realizarlas sin tener que parar el sistema, y mantener el 100\% de tiempo de actividad en nuestro sistema.

Sobre la estructura de particionado, existen muchas formulas, pero una de las posibles seria la siguiente. En primer lugar, una partición para el sistema de arranque. Esto no es estrictamente necesario pero, realmente es una buena práctica poner el sistema de arranque en un tipo de partición más sencilla como ext2 que la partición del sistema principal, que en este caso seria ext4. Yo suelo darle bastante cantidad de espacio, del orden de 10 veces más de lo que necesita. Con 50 Mb es suficiente, pero si en un momento determinado vamos cargando múltiples kernels para cualquier asunto, o actualizaciones sucesivas, y no tener la necesidad de ir borrando lo antiguo, suelo poner unos 500 Mb. Con los discos duros de hoy en día no supone un gasto indebido y nos curamos en salud

En segundo lugar el espacio Swap. Hay que considerar una cosa: Si tenemos que utilizar el sistema Swap en nuestra maquina Asterisk, vamos a tener serios problemas. Yo personalmente me plantearía elegir dedicar 0 Mb. Pero con mi política de reservar espacio para todo y que en un momento dado, aunque el sistema vaya mal, al menos no vaya a pique con facilidad, me gusta reservar tanta memoria Swap como memoria RAM disponible haya.

El resto, volumen principal, en ext4 que es el más reciente en estos momentos, y cumple su función. Los sistemas Ubuntu lo permiten y Asterisk funciona perfectamente en este tipo de sistema.

\section{Virtualización para Asterisk}

Generalmente, siempre que sea posible evitarlo, no es una gran idea Virtualizar Asterisk, por la principal deficiencia que presentan todos los sistemas de Virtualización: la escasa optimización de los recursos por parte de los sistemas virtualizados. Se suele optar por sistemas “sobre el metal” (bare-metal) a la hora de diseñar plataformas Asterisk puesto que es la forma de obtener el máximo exponencial dedicado para su uso.

En contrapartida, la virtualización, ofrece grandes bondades que en ciertos entornos pueden superar ampliamente, a la principal deficiencia. La característica más positiva que nos ofrecen, es la capacidad de restablecer un sistema en tiempo record y que simultáneamente, es un símbolo de protección por su efecto de back-up.

Existen múltiples sistemas de virtualización, pero los mas reconocidos, en el mundo del software libre son tres: OpenVZ, Xen, y KVM. 

OpenVZ surge como evolución del sistema Virtuozzo, extensamente conocido en el mundo de los Alojamientos web. La principal ventaja de OpenVZ es que se integra con el sistema host perfectamente, y una de las limitaciones es que tanto el host como el cliente han de ser sistemas GNU/Linux (esa es la principal razón de esta magnifica integración). 

Por otro lado, Xen, nacido en 2003, es un sistema de virtualización basado en maquinas cuyos sistemas operativos están diseñados específicamente para trabajar sobre él. Ha habido múltiples evoluciones en lo que este requerimiento se ha flexibilizado bastante (específicamente, para sistemas operativos cuyo código no es abierto como Microsoft Windows, para poder “saltar” esta barrera original). Se convirtió en la solución de código libre mas distendida en términos de virtualización.

En última instancia surgió KVM como nuevo competidor, a raíz de que el Kernel de Linux (versión 2.6.20 año 2007) permitía por primera vez utilizar las extensiones Intel-VT y AMD-V de virtualización. Pretendía que entrar en el mercado dominado por VMWare en el mundo propietario, y Xen en el mundo Open Source. Por otro lado también emergían nuevas soluciones como Virtual Box de Sun Microsystems, que a posteriori sería absorbida por Oracle.

En el origen surgían varios inconvenientes, era necesario tener un procesador con soporte para virtualización por hardware, como son los procesadores con tecnología AMD-V e Intel-VT, el número de seguidores evidentemente era limitado, así como el apoyo económico y tecnológico que ofrecían las grandes marcas (RH, Novell, Citrix…) y además surgió exclusivamente como un sistema basado en línea de comandos, bastante complejo, sin posibilidades de adaptarlo a interfaces gráficas de forma relativamente aceptable.

Pero hoy en día todo esto quedó atrás. Por un lado, casi todos los procesadores soportan la virtualización por hardware. A lo mejor maquinas antiguas sufren esto, pero ya es realmente raro mantener un servidor de tal antigüedad con semejantes características, y ni siquiera plantearse el hecho de mandarlo a la guerra de las maquinas virtuales con semejante antigüedad.

Es curioso como la curva de seguidores, en una balanza, lleva tiempo inclinándose a favor de KVM. Desde la lectura de estas líneas KVM ha superado ampliamente a la comunidad de XEN, de hecho XEN cada vez pierde mas “seguidores” a costa de KVM como puede verse en la mayoría de las estadísticas (ejemplo, Google Trends).

Sobre los apoyos tecnológicos, económicos, y técnicos, la verdad es que KVM empezó a gozar del apoyo de uno de los principales actores en el ámbito GNU/Linux, RedHat, y desde entonces empieza a hacerle frente incluso a VMWare

Y conjunto a esto, también han surgido las interfaces graficas para dar un apoyo definitivo e impulso al sistema KVM. También surgieron distribuciones específicas de Linux, como Proxmox, que implementan eficazmente, maquinas virtuales KVM y las dotan de complementos muy interesantes para facilitar la vida al gestor.

Hay que decir para terminar que KVM es de los pocos sistemas de virtualización que soporta el mecanismo PCI Passthrough (paso directo), esto ofrece, la opción de poder soportar tarjetas PCI “exóticas”, como tarjetas de sonido, o lo que aquí nos atañe, tarjetas del sistema Asterisk, de forma nativa, sin tener que manipular la información que circula entre ellas y la máquina. Con esto, a Asterisk se le ofrece la capacidad de obtener una fuente de sincronización pura, y podrían verse las funciones que hasta la fecha eran imposibles o demasiado complejas para verse implementadas directamente en maquinas virtuales.

Concluyendo, para este PFC, se ofrecerá una máquina virtual de ejemplo, conteniendo un sistema Asterisk y toda su funcionalidad, aprovechando todo lo descrito hasta aquí, y esta será creada a partir del sistema KVM.

\section{Instalación de la Máquina Virtual}

Como he comentado, vamos a utilizar el sistema KVM sobre nuestro Ubuntu instalado (podría instalarse en cualquier sistema de hecho, y aplicarse la MV para cualquier entorno dado). A continuación describo el proceso de configuración del mismo:

En el caso de haber elegido Ubuntu, como es el caso aquí descrito, existe la posibilidad de realizar la instalación completa de KVM a través de tasksel, con la opción Virtual Machine Host, que engloba todos los paquetes apt asociados a este sistema de virtualización.

Los paquetes principales son:

\begin{enumerate}
\item \textbf{QEMU-KVM}: Los sistemas de virtualización KVM basado en QEMU(forma parte del proceso de virtualización de KVM)
\item \textbf{Libvirt-bin}: Librerías encargadas de administrar KVM y QEMU
\item \textbf{Ubuntu-VM-Builder}: Herramienta de línea de comandos para administrar maquinas virtuales basadas en KVM
\item \textbf{Bridge-Utils}: Proporciona la conexión “puente” entre la red que proviene del sistema host y las maquinas virtuales. Este es el clásico método de compartir la conexión de forma autónoma y aislada para las maquinas virtuales, en el mundo de la virtualización.
\end{enumerate}

El sistema de conexión puente, aun así no funciona por defecto en la instalación de KVM, y requiere de QEMU para operar, y para la instalación de nuestra maquina Asterisk será importante que tenga un acceso independiente. Para ello tenemos que instalar más paquetes:

\begin{enumerate}
\item \textbf{QEMU}: Sistema de virtualización
\item \textbf{Libcap2-bin}: Librerías de las herramientas de capacidades de Linux, setcap, sucap, etc...
\end{enumerate}

Y con el siguiente comando adquirimos la propiedad CAP\_NET\_ADMIN necesaria para esta funcionalidad de acceso puente:

\textbf{setcap cap\_net\_admin=ei /usr/bin/qemu-system-x86\_64}

Por otro lado, necesitamos configurar la interfaz de red para que funcione como posible puente para nuestras maquinas virtuales. Dentro del fichero de configuración, en caso de Debian /etc/network/interfaces hay que hacer algunas modificaciones para adaptar la interfaz (eth0 clásicamente) y crear el puente br0. A continuación un posible ejemplo:

\begin{lstlisting}[language=bash]
auto eth0
iface eth0 inet manual
auto br0
iface br0 inet static
        address 192.168.1.200
        netmask 255.255.255.00
        gateway 192.168.1.1
        network 192.168.1.0
        broadcast 192.168.1.255
        bridge_ports eth0
        bridge_stp off
        bridge_fd 0
        bridge_maxwait 0
\end{lstlisting}

Realmente ya no es necesario mucho más que esto a nivel de configuración, a partir de aquí ya solo queda la creación de las máquinas. Existen múltiples métodos, y con el tiempo como comentaba antes, sobre las virtudes de KVM, se han desarrollado no solo interfaces graficas, sino sistemas para la línea de comandos que facilitan la creación enormemente. A nivel command-line puedo destacar dos principales: Virt-Manager para la creación de maquinas virtuales en general, y Ubuntu-VM-Builder, que nos interesa concretamente para lo que nos atañe con este proyecto.

Con Ubuntu-VM-Builder (vmbuilder) tenemos la posibilidad de generar una nueva maquina virtual, basada en una versión de Ubuntu a nuestra voluntad, y con una instalación totalmente desatendida. Es lo mas parecido a una maquina virtual con OpenVZ que nos podemos encontrar para KVM. Este “asistente” fue diseñado por Canonical específicamente para Ubuntu, he ahí su potencial. Para la creación con este sistema tenemos dos opciones:

\begin{enumerate}
\item Mediante la creación de un fichero de texto, que utilizaremos en conjunto al comando para que lo tome por defecto. Esta opción es muy interesante, si tenemos intención de diseñar maquinas nuevas en un futuro, y no queremos pasar por el tedioso estado de tener que releer toda la información especifica acerca de esta aplicación. Es de alguna forma, un sistema recordatorio para que en el futuro veamos que parámetros elegimos para nuestras maquinas virtuales
\item Hay otra opción, mas rápida, sencilla y directa, y es aplicando el comando con todas sus opciones. Y es la que voy a aplicar para este caso.
\end{enumerate}

Las características que voy buscando en la maquina a crear para este PFC son las siguientes:

\begin{enumerate}
\item Maquina basada en Ubuntu, versión 11.10 (Oneiric Ocelot)
\item Arquitectura AMD64
\item Dirección IP: 192.168.1.100
\item Puerta de Enlace: 192.168.1.1
\item Usuario: asterisk
\item Contraseña: asterisk
\item Memoria RAM: 2 Gb
\item Disco Duro: 4,5 Gb (un DVD completo en el que alojare la maquina virtual final)
\item La red ira a través de la interfaz puente que creamos anteriormente
\end{enumerate}

Vamos al directorio donde queremos “instalar” la maquina virtual. Un ejemplo clásico seria el directorio especifico de libvirt: /var/lib/libvirt/images/, supongamos que creamos aquí un directorio llamado \negrita{pfc-asterisk} para contener nuestra maquina virtual. 

Para definir el particionado del disco virtual a crear, dentro de este directorio vamos a crear un fichero llamado vmbuilder.partition con la siguiente información:

\begin{lstlisting}[language=bash]
root 4608
\end{lstlisting}

Y por otro lado, para asegurarnos una conexión sencilla mediante acceso SSH, vamos a crear un fichero que se ejecutara en el primer arranque de nuestro sistema, en el mismo directorio también, un fichero llamado boot.sh

\begin{lstlisting}[language=bash]
apt-get update
apt-get install -qqy --force-yes openssh-server
\end{lstlisting}

Entonces el comando quedaría así:

\begin{lstlisting}[style=consola]
vmbuilder kvm ubuntu --suite=oneiric --flavour=virtual --arch=amd64 
--mirror=http://de.archive.ubuntu.com/ubuntu -o --libvirt=qemu:///system 
--ip=192.168.1.100 --gw=192.168.1.1 --part=vmbuilder.partition 
--user=asterisk --name=Asterisk --pass=asterisk --addpkg=unattended-upgrades 
--addpkg=acpid --addpkg=aptitude 
--firstboot=/var/lib/libvirt/images/pfc-asterisk/boot.sh --mem=2048 
--hostname=pfc-asterisk -bridge=br0
\end{lstlisting}

Con el comando \negrita{vmbuilder kvm ubuntu --help} tenemos acceso  al significado de cada uno de los parámetros aquí utilizados aunque 

Una vez terminado el proceso, que no debe tardar demasiado, ya solo queda arrancar el sistema, a través de la consola de maquinas virtuales, Virsh.

\begin{lstlisting}[style=consola]
# virsh --connect qemu:///system
virsh # list --all
virsh # start pfc-asterisk
\end{lstlisting}

Desde este momento, y pasado el rato que termina el proceso de arranque de Ubuntu, podríamos acceder con cualquier cliente SSH a nuestra maquina virtual apuntando a la dirección que habíamos definido en las opciones del ejemplo: 192.168.1.100, y debería conectar sin ningún problema. 

El fichero de configuración con toda la información con la que hemos diseñado esta maquina virtual (y que podríamos editar a voluntad), se encuentra ubicado en la ruta, /etc/libvirt/qemu/pfc-asterisk.xml

Este fichero es muy práctico, dado que es el fichero que nos facilita la portabilidad para mover nuestra máquina (junto evidentemente, al disco duro virtual ubicado en la anterior ruta) a otra maquina física, e instalando todo el sistema KVM, tenerla activa en solo unos minutos, con total abstracción de los drivers específicos de la nueva máquina. He aquí donde realmente radica el verdadero potencial de las maquinas virtuales.

Tenemos múltiples opciones para trabajar con las distintas aplicaciones de manejo y gestión de maquinas virtuales, y con la creación de las mismas concretamente basadas en KVM-QEMU, pero este resumen es en líneas generales, el procedimiento seguido por mi parte enfocando en el propósito especifico de este proyecto.

\figura{capacidad_inicial.png}{scale=1}{Capacidad inicial del disco duro en la Maquina Virtual}{capacidad_inicial}{H}